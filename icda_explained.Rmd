---
title: "icda_explained"
author: "Alokesh Manna"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("devtools")
library(devtools)
#install_github("presnell/icda")
library(icda)
```
Cola preference
```{r}
dbinom(0, 3, .6)
dbinom(1, 3, .6)
dbinom(0:3, 3, .6)
cbind(0:3, dbinom(0:3, 3, .6))


###################################################
### code chunk number 9: icda-notes.Rnw:490-492
###################################################
```
1. How to plot a PMF? type = "h"
```{r}
plot(0:3, dbinom(0:3, 3, .6), type = "h",
     xlab = "y", ylab = "P(y)")


```
2. Plot a overall distribution
```{r}
opar <- par(mfrow=c(1,2))
plot(0:8, dbinom(0:8,8,.2), type="h", xlab="y", ylab="p(y)",
main=expression(paste("Binomial(n=8, ",pi,"=.2)")))
plot(0:15, dbinom(0:15,25,.2), type="h", xlab="y", ylab="p(y)",
main=expression(paste("Binomial(n=25, ",pi,"=.2)")))
par(opar)

```
3. A curve? curve has an argument as x. Also we want to show different segments.
```{r}
curve(dbinom(1,3,x), xlim = c(0,1),
xlab=expression(paste("Binomial parameter: ",pi)),
ylab=expression(paste("Likelihood:  ", l(pi))))

curve(dbinom(1,3,x), xlim = c(0,1), xaxt="n",
      xlab=expression(paste("Binomial parameter: ",pi)),
      ylab=expression(paste("Likelihood: ", l(pi))))
axis(1, at=c(0,1/3,1), labels=c("0","1/3","1"))
segments(1/3, 0, 1/3, dbinom(1, 3, 1/3), lty=2)

curve(dbinom(3,3,x), xlim = c(0,1),
      xlab=expression(paste("Binomial parameter: ",pi)),
      ylab=expression(paste("Likelihood: ", l(pi))))
segments(1, 0, 1, dbinom(3, 3, 1), lty=2)


```
4.
Null: x/n = .5 if there is only one group.

prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)
          
CI is constructed based on normality assumption. It keeps [-1,1] or
[0,1] depending on whether the test is for 1 or 2 group. 

Continuity correction is used only if it does not exceed the 
difference between sample and null proportions in absolute value. 

```{r}
prop.test(0,20)
```
For very small n, do inference using the exact binomial sampling
distribution of the data, instead of the normal approximation.

R Functions for Simple Binomial Tests and CI
```{r}
binom.test(0,20)
```
5. contingency table from page 35 <https://huskyct.uconn.edu/ultra/courses/_150350_1/cl/outline>

```{r}
phs <- matrix(c(189, 10845, 104, 10933), ncol = 2, byrow = TRUE)
dimnames(phs) <- 
  list(Group = c("Placebo","Aspirin"), MI = c("Y", "N"))

```


```{r}
phs
```
If you get a positive result, more relevant to you is Pr(X = 1|Y = 1). If
disease is relatively rare, this may be low even if sensitivity and
specificity are high.

Statistical independence test:
1. direct method:
$SE(p1 − p2) = \sqrt{p1(1 − p1)/n1+ p2(1 − p2)/n2}$
use the normality and CI approach.
If $p1 − p2+/-SE(p1 − p2)$ includes 0, no significant difference.

2.$\text{odd ratio} =(π1/(1 − π1))/(π2/(1 − π2))=π1(1 − π2)/π2(1 − π1)$
Test for $\text{odd ratio}=1$ or not!
$SE(log odd ratio) = \sqrt{1/n11+1/n12+1/n21+1/n22}$
So now use the CI $log odd ratio+/-1.96 SE$

3. Chi square independence test. df:(I-1)(J-1). one sided test.

4. Liklihood ration test. $-2log(\text{liklihood under} H_{0}/\text{liklihood under} H_{1})$. df:(I-1)(J-1). one sided test.

```{r}
pag.tab <- matrix(c(762, 484, 327, 239, 468, 477), nrow=2)
dimnames(pag.tab) <- 
  list(Gender=c("Female","Male"),
       Party=c("Democrat","Independent","Republican"))
pag.tab <- as.table(pag.tab)
pag.tab


###################################################
### code chunk number 36: icda-notes.Rnw:1965-1967
###################################################
pag.df <- as.data.frame(pag.tab)
pag.df


###################################################
### code chunk number 37: icda-notes.Rnw:1977-1982
###################################################
pag.df <- 
  expand.grid(Gender=c("Female","Male"),
             Party=c("Democrat","Independent","Republican"))
pag.df
pag.df$Freq <- c(762, 484, 327, 239, 468, 477)


###################################################
### code chunk number 38: icda-notes.Rnw:1990-1992
###################################################
pag.df
xtabs(Freq ~ Gender + Party, data=pag.df)


###################################################
### code chunk number 39: icda-notes.Rnw:2008-2009 (eval = FALSE)
###################################################
## pag.df <- read.table("Data/pag.txt", header=TRUE)


```
```{r}
pag.tab
margin.table(pag.tab, 1)
margin.table(pag.tab, 2)


###################################################
### code chunk number 42: icda-notes.Rnw:2042-2043
###################################################
addmargins(pag.tab)


###################################################
### code chunk number 43: icda-notes.Rnw:2054-2056
###################################################
prop.table(pag.tab)
round(prop.table(pag.tab), 3)


###################################################
### code chunk number 44: icda-notes.Rnw:2066-2067
###################################################
prop.table(pag.tab, 1) # row wise proportion


###################################################
### code chunk number 45: icda-notes.Rnw:2071-2072
###################################################
prop.table(pag.tab, 2)# column wise proportion

```

chisquare test
```{r}
chisq.test(pag.tab)


###################################################
### code chunk number 47: icda-notes.Rnw:2091-2096
###################################################
pag.chisq <- chisq.test(pag.tab)
names(pag.chisq)
pag.chisq$statistic
pag.chisq$parameter
pag.chisq$p.value
```
```{r}
###################################################
pag.chisq$observed
pag.chisq$expected
with(pag.chisq, sum((observed - expected)^2/expected))

```
we can check the residuals and standarized residuals
```{r}
###################################################
pag.chisq$residuals


###################################################
### code chunk number 50: icda-notes.Rnw:2125-2126
###################################################
pag.chisq$stdres

```
```{r}
```


Now one can check for 72-75 with 3*3 table.
```{r}

```


Exact Inference for Small Samples

2 × 2 Case: Fisher’s Exact Test: when row and column total are fixed
we should use hypergeometric distribution.

1. Lady tasting tea:The lady is told that milk was poured first in 4 cups and tea first in the other 4. Order of tasting is randomized. Asked to identify the 4 cups
with milk poured first.

```{r}
x <- cbind(0:4, round(dhyper(0:4, 4, 4, 4), 3))
colnames(x) <- c("$n_{11}$", "$P(n_{11})$")
#latex(x, file="", first.hline.double=FALSE)


###################################################
### code chunk number 57: icda-notes.Rnw:2459-2466
###################################################
TeaTasting <-
  matrix(c(3, 1, 1, 3),
         nrow = 2,
         dimnames = list(Truth = c("Milk", "Tea"),
           Guess = c("Milk", "Tea")))
TeaTasting <- as.table(TeaTasting)
TeaTasting
```

fisher test used odd ratio.

```{r}
fisher.test(TeaTasting, alternative = "greater")


###################################################
### code chunk number 58: icda-notes.Rnw:2486-2487
###################################################
fisher.test(TeaTasting, alternative = "two.sided")


###################################################
### code chunk number 59: icda-notes.Rnw:2511-2513
###################################################
#sattab
#fisher.test(sattab)


```


This can be extended to more than 2*2 table!

```{r}
library(icda)
data(deathpenalty)
deathpenalty <-
  transform(deathpenalty,
            DeathPenalty = relevel(DeathPenalty, "Yes"),
            Defendant = relevel(Defendant, "White"),
            Victim = relevel(Victim, "White"))
deathpenalty
```
Make a table from the data
```{r}
dp <- xtabs(Freq ~ Victim + Defendant + DeathPenalty, 
            data=deathpenalty)
dp

```
Make it so that it looks better.
```{r}
dpflat <- ftable(DeathPenalty ~ Victim + Defendant, 
                 data=dp)
dpflat

```

```{r}
round(100*prop.table(dpflat,1), 1)
```
These are called partial tables. They control for Z (hold it constant).
Y = death penalty (response var.)
X = defendant’s race (explanatory)
Z = victim’s race (control var.)
The (estimated) conditional odds ratios are:
```{r}
dpmargin <- xtabs(Freq ~ Defendant + DeathPenalty,
                  data=deathpenalty)
dpmargin
```

The (estimated) conditional odds ratios are:
Z = white : odds XY(1) =(53 × 37)/(414 × 11)= 0.43 (0.42 after add .5 to all cells)
Z = black : odds XY(2) = (0 × 139)/(16 × 4)= 0 (0.94 after add .5 to all cells)
Controlling for victim’s race, odds of receiving death penalty were lower
for white defendants than for black defendants.

Interesting takeaway:
1. 
Very interesting! without adding .5, the conclusion would have differed!!

2.
Simpson’s Paradox:

Now ignore race! the odds ratio is 1.45

can be dangerous to “collapse” contingency tables

3. Conditional Independence

X and Y are conditionally independent given Z if they are independent in
each partial table.

Null: all odd ratio are 1.

1. Cochran-Mantel-Haenszel (CMH) Test

```{r}
# Create a 3-way contingency table
data <- array(c(23, 12, 18, 25, 16, 28, 30, 10, 15, 35, 20, 40),
              dim = c(2, 2, 3),
              dimnames = list(A = c("Yes", "No"), 
                              B = c("Yes", "No"), 
                              C = c("Group1", "Group2", "Group3")))

# Print the contingency table
print(data)

# Conduct the CMH test for conditional independence between A and B, controlling for C
cmh_result <- mantelhaen.test(data)

# Print the result
print(cmh_result)

```

Can also be extended to generalized linear model:

```{r}
# Example contingency table (3-way)
data <- array(c(23, 12, 18, 25, 16, 28, 30, 10, 15, 35, 20, 40),
              dim = c(2, 2, 3),
              dimnames = list(A = c("Yes", "No"), 
                              B = c("Yes", "No"), 
                              C = c("Group1", "Group2", "Group3")))

# Convert to table object
table_data <- as.table(data)

# Load the necessary library
library(MASS)

# Fit the log-linear model without interaction A:B
model <- loglm(~ A + B + C + A:C + B:C, data = table_data)

# Print the summary
summary(model)

```





Generalized linear model:

The same ML (maximum likelihood) fitting procedure applies to all
GLMs. It is the basis of the glm() function in R and of proc
genmod in SAS.

```{r}

data(malformation)
malformation

```

```{r}
malform.tab <- xtabs(Freq ~ Alcohol + Malformation, 
                     data=malformation)
malform.tab
round(100*prop.table(malform.tab, 1), 2)


###################################################
### code chunk number 69: malformation.3
###################################################
library(reshape2)
malformwide <- dcast(malformation,
                   Alcohol ~ Malformation, 
                   value.var="Freq")
malformwide


###################################################
### code chunk number 70: malformation.4
###################################################
malform.lin <- 
  glm(cbind(Present,Absent) ~ Alcohol, 
      family=binomial(link=make.link("identity")),
      data=malformwide)
malformwide <- 
  transform(malformwide, Total = Present + Absent)
malform.lin.alt <- 
  glm(Present/Total ~ Alcohol, weights=Total,
      family=binomial(link=make.link("identity")),
      data=malformwide)
coef(malform.lin)

```


```{r}
 summary(malform.lin)
```

```{r}
###################################################
malcoef.lin <- summary(malform.lin)$coefficients


###################################################
### code chunk number 74: malform.6
###################################################
malform.logit <- glm(cbind(Present,Absent) ~ Alcohol, 
                     family=binomial, data=malformwide)


###################################################
### code chunk number 75: malform.7 (eval = FALSE)
###################################################
## summary(malform.logit)


###################################################
### code chunk number 76: icda-notes.Rnw:3016-3017
###################################################
summary(malform.logit)


```
Takeaway:

1. p-value = 0.012 for H0 : beta = 0 vs Ha : beta not equal to 0.
But p-value = 0.3 if delete single “present” obs. in >=6 drink row!


```{r}
malcoef.logit <- summary(malform.logit)$coefficients
maldel <- malformwide
maldel[5,c("Present","Total")] <- c(0,37)
maldel.logit <- 
  glm(Present/Total ~ Alcohol, weights=Total,
      family=binomial(link="logit"),
      data=maldel)
maldel.pval <- summary(maldel.logit)$coefficients[2,4]

```

Upto slide 110
```{r}
###################################################
opar <- par(mfrow=c(1,2))
plot(0:10, dpois(0:10,2.25), type="h", xlab="y", ylab="p(y)",
     main=expression(paste("Poisson(", mu, "=2.25)")))
plot(0:18, dpois(0:18,7.3), type="h", xlab="y", ylab="p(y)",
     main=expression(paste("Poisson(", mu, "=7.3)")))
par(opar)

```

2. Another example

Defects in Silicon Wafers
Y = number defects on silicon wafer
x = dummy var. for treatment (0 = A, 1 = B)

```{r}
A <- c(8,7,6,6,3,4,7,2,3,4)
B <- c(9,9,8,14,8,13,11,5,7,6)
trt <- factor(rep(c("A","B"), each=10))
wafers <- data.frame(trt=trt, defects=c(A,B))
wafers.lin <- glm(defects ~ trt, 
                 family=poisson(link="identity"),
                 data=wafers)
wafers.loglin <- glm(defects ~ trt, 
                     family=poisson(link="log"),
                     data=wafers)


###################################################
### code chunk number 81: wafers.2 (eval = FALSE)
###################################################
## summary(wafers.lin)


###################################################
### code chunk number 82: wafers.3 (eval = FALSE)
###################################################
## summary(wafers.loglin)


###################################################
### code chunk number 83: icda-notes.Rnw:3180-3181
###################################################
summary(wafers.lin)


###################################################
### code chunk number 84: icda-notes.Rnw:3188-3189
###################################################
summary(wafers.loglin)

```



```{r}
###################################################
wafercoef.lin <- summary(wafers.lin)$coefficients
wafercoef.loglin <- summary(wafers.loglin)$coefficients

wafcoef <- summary(wafers.loglin)$coefficients
wafL1 <- 
  with(wafers.loglin,
       sum(y*log(fitted.values) - fitted.values - lfactorial(y)))
wafL0 <-
  with(update(wafers.loglin, . ~ 1),
       sum(y*log(fitted.values) - fitted.values - lfactorial(y)))
wafdev <- wafers.loglin$deviance
wafdf <- wafers.loglin$df.residual
wafdev0 <- wafers.loglin$null.deviance
wafdf0 <- wafers.loglin$df.null


#############################################
#######################################
drop1(wafers.loglin, test="Chisq") 


###################################################
### code chunk number 88: icda-notes.Rnw:3379-3380
###################################################
anova(wafers.loglin, test="Chisq")


```
```{r}
wafCI.LR <- confint(wafers.loglin)
wafCI.Wald <- confint.default(wafers.loglin)

wafCI.LR <- confint(wafers.loglin)
wafCI.Wald <- confint.default(wafers.loglin)
wafCI.LR
exp(wafCI.LR)
wafCI.Wald
```

```{r}
###################################################
plot(0:53, dpois(0:53,25), type="h", xlab="y", ylab="p(y)",
     main=expression(paste("Poisson(", mu, "=25)")))


```



##Gamma coefficient

```{r}
#install.packages("DescTools")
library(DescTools)
# Example data: Two ordinal variables
x <- c(1, 2, 3, 4, 5)
y <- c(1, 3, 2, 4, 5)
# Compute Gamma coefficient
gamma_result <- GoodmanKruskalGamma(x, y)

# Print the result
print(gamma_result)

```


```{r}
summary(gamma_result)
```

McNemar's test

If prime minister has better performance.

2 time points, approve or disapprove. [1,1] is common approval. Did it change? i.e. (n12+n11/n)-(n21+n11/n)=(n12/n)-(n21/n)

The null is that the probabilities of being classified into cells [i,j] and [j,i] are the same.

If x is a matrix, it is taken as a two-dimensional contingency table, and hence its entries should be nonnegative integers. Otherwise, both x and y must be vectors or factors of the same length. Incomplete cases are removed, vectors are coerced into factors, and the contingency table is computed from these.

```{r}
# Create a 2x2 matrix (contingency table)
# Example: 50 individuals, where the rows represent pre-test, and columns represent post-test
# [a b]
# [c d]
# a = no change (No-No), b = before No, after Yes, c = before Yes, after No, d = no change (Yes-Yes)
table_data <- matrix(c(30, 10, 5, 25), nrow = 2,
                     dimnames = list("Pre-test" = c("No", "Yes"),
                                     "Post-test" = c("No", "Yes")))

# View the table
print(table_data)

```

```{r}
# Perform McNemar's test
mcnemar_test_result <- mcnemar.test(table_data)

# View the result
print(mcnemar_test_result)

```
This p-value is used to determine whether there is a significant difference between the paired responses. If the p-value is less than the significance level (usually 0.05), you can reject the null hypothesis and conclude that there is a significant difference between the two time points or conditions.

For exact test

```{r}
# Perform McNemar's test without continuity correction
exact_mcnemar_test_result <- mcnemar.test(table_data, correct = FALSE)

# View the result
print(exact_mcnemar_test_result)

```

